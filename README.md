### sankaku.py:
输入起始月份和结束月份即可开始爬取

网站对访问频率限制较大，我设置为4个协程，总体下载速度偏慢

### id.txt:
用以保存已爬取的id，避免重复爬取

### href.txt:
用以保存已解析的url

### fail_sankaku.txt:
用以保存下载失败的url，建议手动复制进浏览器打开

### exe文件夹:
有打包好的exe程序，无需安装Python环境也可使用

- - - -

#### 2020年4月1日更新(1.1):
- 新增 ```file_type()``` 函数，对下载后的文件进行分类
- 新增 ```run_main()``` 函数，捕获连接失败异常并在异常后恢复 ```id.txt```

#### 2020年4月2日更新(1.2):
- 新增 ```rewrite()``` 函数，用以恢复 ```id.txt```
- ```get_href()``` 函数新增解析失败重新解析功能
- 修改部分文字表述